import asyncio
import json
import random
import time

from playwright.async_api import async_playwright, TimeoutError
from bs4 import BeautifulSoup

# --- –ù–û–í–ê–Ø –ò –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π. –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏.
CONCURRENCY_LIMIT = 3
# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
RETRY_COUNT = 3
# –ù–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö). –ë—É–¥–µ—Ç —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å—Å—è.
RETRY_DELAY = 5
# –ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞
BASE_URL = "https://planetazdorovo.ru"


# --- –•–µ–ª–ø–µ—Ä-—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---
def scrape_products_from_page(page_content, base_url):
    """–ü–∞—Ä—Å–∏—Ç HTML-–∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö."""
    soup = BeautifulSoup(page_content, 'html.parser')
    products = []
    product_cards = soup.select('div.pz-grid-list .pz-grid-item .item-card')

    for card in product_cards:
        link_tag = card.select_one('.item-card-title-text')
        title_tag = card.select_one('.item-card-title-text .this-full')
        price_tag = card.select_one('.item-card-price-number')
        availability_tag = card.select_one('.item-card-availability-text .this-text-number')

        if link_tag and title_tag:
            relative_link = link_tag.get('href', '')
            full_link = f"{base_url}{relative_link}" if relative_link else "N/A"

            product_data = {
                'title': title_tag.get_text(strip=True),
                'price': price_tag.get_text(strip=True).replace('\n', '').replace(' ', '') if price_tag else "N/A",
                'availability': availability_tag.get_text(strip=True) if availability_tag else "N/A",
                'link': full_link
            }
            products.append(product_data)

    return products


# --- –ù–æ–≤—ã–µ –∏ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

async def goto_with_retries(page, url: str) -> bool:
    """
    –ü–µ—Ä–µ—Ö–æ–¥–∏—Ç –ø–æ URL —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–ª—É—á–∞–µ –Ω–µ—É–¥–∞—á–∏.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏.
    """
    for i in range(RETRY_COUNT):
        try:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –æ–±—â–∏–π —Ç–∞–π–º–∞—É—Ç –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –¥–æ 90 —Å–µ–∫—É–Ω–¥
            await page.goto(url, wait_until="domcontentloaded", timeout=90000)
            return True # –£—Å–ø–µ—à–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
        except TimeoutError as e:
            print(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {i + 1}/{RETRY_COUNT} –Ω–µ —É–¥–∞–ª–∞—Å—å –¥–ª—è {url}. –û—à–∏–±–∫–∞: {e.message.splitlines()[0]}")
            if i < RETRY_COUNT - 1:
                # –£–≤–µ–ª–∏—á–∏–≤–∞—é—â–∞—è—Å—è –∑–∞–¥–µ—Ä–∂–∫–∞ (5s, 10s, 15s...)
                delay = RETRY_DELAY * (i + 1)
                print(f"   ...–ø–∞—É–∑–∞ –Ω–∞ {delay} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                await asyncio.sleep(delay)
    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {url} –ø–æ—Å–ª–µ {RETRY_COUNT} –ø–æ–ø—ã—Ç–æ–∫.")
    return False # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å

async def get_last_page_number(page) -> int:
    """–ù–∞—Ö–æ–¥–∏—Ç –Ω–æ–º–µ—Ä –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏."""
    try:
        page_links = await page.query_selector_all("div.pagination a.pagination__item")
        if not page_links:
            return 1
        last_page_number = 0
        for link in page_links:
            text = await link.inner_text()
            if text.isdigit() and int(text) > last_page_number:
                last_page_number = int(text)
        print(f"INFO: –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {last_page_number}")
        return last_page_number if last_page_number > 0 else 1
    except Exception as e:
        print(f"WARNING: –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {e}. –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–∞—è.")
        return 1

async def scroll_to_bottom(page):
    """–ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–æ —Å–∞–º–æ–≥–æ –Ω–∏–∑–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö '–ª–µ–Ω–∏–≤—ã—Ö' —ç–ª–µ–º–µ–Ω—Ç–æ–≤."""
    last_height = await page.evaluate("document.body.scrollHeight")
    while True:
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
        try:
            await page.wait_for_load_state('networkidle', timeout=7000)
        except TimeoutError:
            print("...–¢–∞–π–º-–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è 'networkidle', –ø—Ä–æ–¥–æ–ª–∂–∞—é –ø—Ä–æ–∫—Ä—É—Ç–∫—É...")
        await asyncio.sleep(random.uniform(1.5, 2.5))
        new_height = await page.evaluate("document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height

async def scrape_single_category(context, category_url: str):
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é —Å–∫—Ä–µ–π–ø–∏—Ç –æ–¥–Ω—É –∫–∞—Ç–µ–≥–æ—Ä–∏—é, –≤–∫–ª—é—á–∞—è –≤—Å–µ –µ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    print(f"\n--- üöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_url} ---")
    page = await context.new_page()
    all_products_in_category = []

    try:
        # 1. –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–π –Ω–∞–¥–µ–∂–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
        if not await goto_with_retries(page, category_url):
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–∂–µ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.")

        await page.wait_for_selector('div.pz-grid-list', timeout=30000)
        last_page = await get_last_page_number(page)

        # 2. –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        for page_num in range(1, last_page + 1):
            page_url = f"{category_url}?PAGEN_1={page_num}"
            print(f"üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num}/{last_page} –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_url}")

            # –î–ª—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –º—ã —É–∂–µ –Ω–∞ –º–µ—Å—Ç–µ, –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö - –ø–µ—Ä–µ—Ö–æ–¥–∏–º
            if page_num > 1:
                if not await goto_with_retries(page, page_url):
                    print(f"   –ü—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num} –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏.")
                    continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É, –Ω–æ –Ω–µ –≤—Å—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é

            await page.wait_for_selector('div.pz-grid-list', timeout=30000)
            
            print("...–ü—Ä–æ–∫—Ä—É—á–∏–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤...")
            await scroll_to_bottom(page)
            print("...–ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

            content = await page.content()
            products_on_page = scrape_products_from_page(content, BASE_URL)
            all_products_in_category.extend(products_on_page)
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(products_on_page)} —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}.")
            # –£–≤–µ–ª–∏—á–µ–Ω–∞ –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
            await asyncio.sleep(random.uniform(2.0, 4.0))

    except Exception as e:
        print(f"‚ùå CRITICAL ERROR –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {category_url}: {e}")
    finally:
        await page.close()
    
    print(f"--- ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ: {category_url} | –°–æ–±—Ä–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(all_products_in_category)} ---")
    return all_products_in_category

# --- –ì–ª–∞–≤–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –∫—Ä–æ–º–µ –≤—ã–∑–æ–≤–∞ `main`) ---
async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=False,
            args=["--disable-blink-features=AutomationControlled", "--start-maximized"]
        )
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
            viewport={"width": 1920, "height": 1080}
        )

        page = await context.new_page()
        
        print("--- –®–∞–≥ 1: –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ---")
        catalog_url = f"{BASE_URL}/catalog/"
        await page.goto(catalog_url, wait_until="domcontentloaded")
        await page.wait_for_selector("div.catalog", timeout=30000)

        category_links = await page.eval_on_selector_all(
            'div.catalog a.catalog__card',
            'nodes => nodes.map(n => n.href)'
        )
        await page.close()
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(category_links)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞.")

        semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)
        tasks = []

        async def worker(link):
            async with semaphore:
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—É–∑—É –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                await asyncio.sleep(random.uniform(3.0, 6.0))
                return await scrape_single_category(context, link)

        for link in category_links:
            tasks.append(worker(link))
            
        results_from_categories = await asyncio.gather(*tasks)

        all_products = [product for sublist in results_from_categories for product in sublist]
        
        print(f"\n--- ‚ú® –°–∫—Ä–µ–π–ø–∏–Ω–≥ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–µ—Ä—à–µ–Ω ---")
        print(f"üéâ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(all_products)}")

        if all_products:
            with open("all_products_async.json", "w", encoding="utf-8") as f:
                json.dump(all_products, f, indent=4, ensure_ascii=False)
            print("üíæ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª 'all_products_async.json'")
        
        await browser.close()
        print("–ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä—ã—Ç.")

if __name__ == "__main__":
    asyncio.run(main())