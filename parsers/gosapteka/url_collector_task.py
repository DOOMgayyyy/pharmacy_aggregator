# parsers/gosapteka/url_collector_task.py
import asyncio
import json
import os
import random
from urllib.parse import urljoin, urlparse, parse_qs, urlunparse, urlencode
import httpx
from bs4 import BeautifulSoup
from config import CONCURRENCY_LIMIT, DELAY_BETWEEN_PAGES, DELAY_BETWEEN_CATEGORIES, URLS_DIR

# --- –ë–ê–ó–û–í–´–ô –ö–õ–ê–°–° (–û–±—ã—á–Ω–æ –≤—ã–Ω–æ—Å–∏—Ç—Å—è, –Ω–æ –∑–¥–µ—Å—å –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã) ---
class GosAptekaParser:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–µ—Ä–æ–≤ –ì–æ—Å–∞–ø—Ç–µ–∫–∏."""
    def __init__(self, session: httpx.AsyncClient):
        self.base_url = 'https://gosapteka18.ru'
        self.session = session

    async def fetch_html(self, url: str) -> str | None:
        try:
            await asyncio.sleep(0.5)
            response = await self.session.get(url, timeout=20, follow_redirects=True)
            response.raise_for_status()
            return response.text
        except httpx.RequestError as e:
            print(f"üö´ –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None

# --- –ö–õ–ê–°–° –î–õ–Ø –°–ë–û–†–ê –°–¢–†–£–ö–¢–£–†–´ –ö–ê–¢–ï–ì–û–†–ò–ô ---
class CategoryStructureParser(GosAptekaParser):
    """–ö–ª–∞—Å—Å –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Å–±–æ—Ä–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å —Å–∞–π—Ç–∞."""
    async def parse_structure(self) -> dict:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∫–∞—Ç–∞–ª–æ–≥–∞.
        """
        print("‚ñ∂Ô∏è –®–∞–≥ 1: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π...")
        main_page_url = self.base_url + '/'
        html = await self.fetch_html(main_page_url)
        if not html:
            return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É'}

        try:
            soup = BeautifulSoup(html, 'html.parser')
            catalog_container = soup.find('div', class_='menu-catalog')
            if not catalog_container:
                return {'error': "–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –∫–∞—Ç–∞–ª–æ–≥–∞ 'menu-catalog' –Ω–µ –Ω–∞–π–¥–µ–Ω"}

            structured_categories = {}
            columns = catalog_container.find_all('div', class_='menu-catalog__list')
            if not columns:
                return {'error': "–ö–æ–ª–æ–Ω–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π 'menu-catalog__list' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"}

            for col in columns:
                items = col.find_all('div', class_='menu-catalog__item', recursive=False)
                for item in items:
                    parent_link = item.find('a', class_='menu-catalog__link')
                    if not parent_link or not parent_link.text.strip():
                        continue

                    parent_name = parent_link.text.strip()
                    parent_url = self.base_url + parent_link.get('href', '')
                    subcategories_l1 = []
                    submenu_l1 = item.find('div', class_='menu-catalog__sub-menu')
                    
                    if submenu_l1:
                        subitems_l1 = submenu_l1.find_all('div', class_='menu-catalog__sub-item')
                        for subitem in subitems_l1:
                            sub_link = subitem.find('a', class_='menu-catalog__sub-link')
                            if not sub_link: continue
                            
                            sub_name = sub_link.text.strip()
                            sub_url = self.base_url + sub_link.get('href', '')
                            subcategories_l2 = []
                            submenu_l2 = subitem.find('div', class_='menu-catalog__sub2-menu')
                            
                            if submenu_l2:
                                for sub2_link in submenu_l2.find_all('a', class_='menu-catalog__sub2-link'):
                                    subcategories_l2.append({
                                        'name': sub2_link.text.strip(),
                                        'url': self.base_url + sub2_link.get('href', '')
                                    })
                            
                            subcategories_l1.append({
                                'name': sub_name, 'url': sub_url, 'subcategories': subcategories_l2
                            })
                    
                    structured_categories[parent_name] = {'url': parent_url, 'subcategories': subcategories_l1}
            
            print("‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —É—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω–∞.")
            return structured_categories
        except Exception as e:
            print(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
            return {"error": str(e)}

# --- –ö–õ–ê–°–° –î–õ–Ø –°–ë–û–†–ê –°–°–´–õ–û–ö –ù–ê –¢–û–í–ê–†–´ ---
class ProductLinkParser(GosAptekaParser):
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å —É—á–µ—Ç–æ–º –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.
    """
    def __init__(self, session: httpx.AsyncClient):
        super().__init__(session)
        self.parsed_links = set()
        self.first_page_content = None

    def extract_links_from_page(self, html: str) -> list[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        if not html: return []
        
        soup = BeautifulSoup(html, 'html.parser')
        products_grid = soup.find('div', class_='products__grid')
        if not products_grid: return []
        
        links = products_grid.select('a.product-mini__title-link, a.product-mini__picture')
        
        return list(set([urljoin(self.base_url, link.get('href')) for link in links if link.get('href')]))

    def find_next_page(self, html: str, current_url: str, current_page: int) -> str | None:
        """–ù–∞—Ö–æ–¥–∏—Ç URL —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏—Å–ø–æ–ª—å–∑—É—è 3 –º–µ—Ç–æ–¥–∞."""
        soup = BeautifulSoup(html, 'html.parser')
        
        next_btn = soup.find('a', class_='modern-page-next')
        if next_btn and next_btn.get('href'):
            return urljoin(self.base_url, next_btn.get('href'))
        
        page_links = soup.select('a.pagination__item, a.bx-pagination-container a')
        for link in page_links:
            try:
                if int(link.text.strip()) == current_page + 1:
                    return urljoin(self.base_url, link.get('href'))
            except (ValueError, TypeError):
                continue
        
        parsed_url = urlparse(current_url)
        query_params = parse_qs(parsed_url.query)
        query_params['PAGEN_1'] = [str(current_page + 1)]
        new_query = urlencode(query_params, doseq=True)
        return urlunparse((parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, new_query, parsed_url.fragment))

    async def parse_products_from_category(self, start_url: str, category_name: str):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ —Å–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã."""
        print(f"\nüöÄ –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category_name}': {start_url}")
        
        current_url = start_url
        page_num = 1
        all_links_for_category = []
        
        while current_url:
            print(f"üìñ –°—Ç—Ä–∞–Ω–∏—Ü–∞ #{page_num}: {current_url}")
            
            html = await self.fetch_html(current_url)
            if not html:
                print("–ü—Ä–µ—Ä—ã–≤–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏.")
                break

            if page_num == 1:
                self.first_page_content = html

            page_links = self.extract_links_from_page(html)
            new_links = [link for link in page_links if link not in self.parsed_links]
            print(f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(page_links)}, –∏–∑ –Ω–∏—Ö –Ω–æ–≤—ã—Ö: {len(new_links)}")
            
            if not page_links or (page_num > 1 and not new_links) or (page_num > 1 and html == self.first_page_content):
                if not page_links or (page_num > 1 and not new_links):
                    print("‚õî –ù–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—É—Å—Ç–∞, –∑–∞–≤–µ—Ä—à–∞—é.")
                else:
                    print("‚ö†Ô∏è  –î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ (–∫–æ–Ω—Ç–µ–Ω—Ç –¥—É–±–ª–∏—Ä—É–µ—Ç –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É).")
                break

            all_links_for_category.extend(new_links)
            self.parsed_links.update(new_links)
            
            next_url = self.find_next_page(html, current_url, page_num)
            
            if not next_url or next_url == current_url:
                print("‚û°Ô∏è  –°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∑–∞–≤–µ—Ä—à–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.")
                break
            
            current_url = next_url
            page_num += 1
            delay = random.uniform(*DELAY_BETWEEN_PAGES)
            await asyncio.sleep(delay)
        
        if all_links_for_category:
            self.save_results(all_links_for_category, start_url, category_name)
        print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category_name}' –∑–∞–≤–µ—Ä—à–µ–Ω! –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {len(all_links_for_category)}")
        return all_links_for_category

    def save_results(self, links: list, base_url: str, category_human_name: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Å—ã–ª–∫–∏ –∏ URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ JSON-—Ñ–∞–π–ª."""
        category_slug = urlparse(base_url).path.strip('/').split('/')[-1] or "home"
        os.makedirs(URLS_DIR, exist_ok=True)
        filepath = os.path.join(URLS_DIR, f"{category_slug}.json")

        output_data = {
            'category_url': base_url,
            'category_name': category_human_name,
            'product_urls': sorted(list(set(links)))
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è '{category_human_name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ JSON: {filepath}")


# --- –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø-–û–†–ö–ï–°–¢–†–ê–¢–û–† ---
async def collect_urls_to_files():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è URL –≤ —Ñ–∞–π–ª—ã."""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
    }
    
    async with httpx.AsyncClient(headers=headers, follow_redirects=True) as session:
        structure_parser = CategoryStructureParser(session)
        catalog_data = await structure_parser.parse_structure()

        if 'error' in catalog_data or not catalog_data:
            print(f"üö´ –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {catalog_data.get('error', '–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞—Ç–µ–≥–æ—Ä–∏–π.')}")
            return
        
        with open('categories_structure.json', 'w', encoding='utf-8') as f:
            json.dump(catalog_data, f, ensure_ascii=False, indent=4)
        print("\nüíæ –ü–æ–ª–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'categories_structure.json'\n" + "="*50)

        product_parser = ProductLinkParser(session)
        semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)
        tasks = []

        async def worker(cat_url, sub_cat_name):
            async with semaphore:
                print(f"‚ú® –ó–∞–ø—É—Å–∫–∞—é –∑–∞–¥–∞—á—É –¥–ª—è –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏: '{sub_cat_name}'")
                await product_parser.parse_products_from_category(cat_url, sub_cat_name)
                delay = random.uniform(*DELAY_BETWEEN_CATEGORIES)
                print(f"--- –ü–∞—É–∑–∞ {delay:.2f} —Å–µ–∫. –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π ---")
                await asyncio.sleep(delay)
        
        for parent_data in catalog_data.values():
            for subcategory in parent_data.get('subcategories', []):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–≤–æ–∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (3-–π —É—Ä–æ–≤–µ–Ω—å)
                if subcategory.get('subcategories'):
                    for sub_sub_cat in subcategory['subcategories']:
                        tasks.append(asyncio.create_task(worker(sub_sub_cat['url'], sub_sub_cat['name'])))
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç, –ø–∞—Ä—Å–∏–º —Å–∞–º—É –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é 2-–≥–æ —É—Ä–æ–≤–Ω—è
                    tasks.append(asyncio.create_task(worker(subcategory['url'], subcategory['name'])))
        
        print(f"\nüî• –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ {len(tasks)} –∑–∞–¥–∞—á –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞. –ó–∞–ø—É—Å–∫–∞—é...")
        await asyncio.gather(*tasks)

    print("\n\nüéâ –í—Å–µ –∑–∞–¥–∞—á–∏ –ø–æ —Å–±–æ—Ä—É URL –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!.")

# –≠—Ç–∞ —á–∞—Å—Ç—å –Ω—É–∂–Ω–∞, –µ—Å–ª–∏ –≤—ã –∑–∞—Ö–æ—Ç–∏—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å —ç—Ç–æ—Ç —Ñ–∞–π–ª –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
if __name__ == "__main__":
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç
    if not os.path.exists(URLS_DIR):
        os.makedirs(URLS_DIR)
        print(f"–°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {URLS_DIR}")
        
    asyncio.run(collect_urls_to_files())